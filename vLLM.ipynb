{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd6bc3a-11df-46b5-ae62-db6954cce1cc",
   "metadata": {},
   "source": [
    "# 下载并直接运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4309b4-bfa8-4e93-b244-e8dc1fc3306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 11:42:18,953 - modelscope - INFO - PyTorch version 2.3.1 Found.\n",
      "2024-06-22 11:42:18,954 - modelscope - INFO - Loading ast index from /home/ubuntu/.cache/modelscope/ast_indexer\n",
      "2024-06-22 11:42:19,011 - modelscope - INFO - No valid ast index found from /home/ubuntu/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-06-22 11:42:19,064 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 e9a35559cccf70bdad8675fad207a115 and a total number of 980 components indexed\n",
      "Downloading: 100%|██████████| 1.23k/1.23k [00:00<00:00, 4.86kB/s]\n",
      "Downloading: 100%|██████████| 73.0/73.0 [00:00<00:00, 312B/s]\n",
      "Downloading: 100%|██████████| 242/242 [00:00<00:00, 937B/s]\n",
      "Downloading: 100%|██████████| 11.1k/11.1k [00:00<00:00, 42.8kB/s]\n",
      "Downloading: 100%|██████████| 1.59M/1.59M [00:00<00:00, 2.42MB/s]\n",
      "Downloading: 100%|██████████| 698M/698M [00:05<00:00, 130MB/s]  \n",
      "Downloading: 100%|██████████| 3.63k/3.63k [00:00<00:00, 14.7kB/s]\n",
      "Downloading: 100%|██████████| 6.70M/6.70M [00:00<00:00, 8.59MB/s]\n",
      "Downloading: 100%|██████████| 1.26k/1.26k [00:00<00:00, 5.46kB/s]\n",
      "Downloading: 100%|██████████| 2.65M/2.65M [00:00<00:00, 4.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('qwen/Qwen2-0.5B-Instruct-GPTQ-Int4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47c10f9-be4e-4c1f-930b-6ac108b365ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/home/ubuntu/miniconda3/envs/qwen2/lib/python3.8/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4 were not used when initializing Qwen2ForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.bias']\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of machine learning algorithm that uses a large amount of data to learn from it. It is typically trained on a large corpus of text, which is then fed into the model to generate new text. The goal of a large language model is to be able to generate human-like language at a high level of accuracy and fluency. This can be achieved by training the model on a large corpus of text, which is then fed into the model to generate new text. The model's output can be used to answer questions about the world, suggest new ideas, and provide information on various topics. Large language models are an important part of many applications in artificial intelligence, including chatbots, natural language processing, and recommendation systems.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff0f54-71dc-4ba3-9bea-6b88fcb70472",
   "metadata": {},
   "source": [
    "# vLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2984298-0574-4433-a398-52ea906537db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac8a2f6-d89a-49a0-b7eb-5073abc8a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b98769-3c87-4c9d-ad62-54b9397214dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-22 12:14:39 config.py:217] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 06-22 12:14:39 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 12:14:39 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-22 12:14:39 selector.py:51] Using XFormers backend.\n",
      "INFO 06-22 12:14:40 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-22 12:14:40 selector.py:51] Using XFormers backend.\n",
      "INFO 06-22 12:14:41 model_runner.py:160] Loading model weights took 0.4318 GB\n",
      "INFO 06-22 12:14:43 gpu_executor.py:83] # GPU blocks: 24881, # CPU blocks: 21845\n",
      "INFO 06-22 12:14:46 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 12:14:46 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 12:14:57 model_runner.py:965] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4\",dtype='half', gpu_memory_utilization=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc449c26-7d03-4268-9403-03a427054d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 16.47it/s, est. speed input: 90.67 toks/s, output: 263.73 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' Jamie, and I am a 21 year old American with a degree in'\n",
      "Prompt: 'The president of the United States is', Generated text: ' your President.\\nThe President of the United States is a member of the executive branch'\n",
      "Prompt: 'The capital of France is', Generated text: ' ________.\\nA. Paris\\nB. Rome\\nC. Madrid\\nD'\n",
      "Prompt: 'The future of AI is', Generated text: ' a little blurry.\\n1. AI is currently being developed in fields such as computer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bbd079-dfd9-4705-b502-91e0cec37cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result: Completion(id='cmpl-b02d9fa51f24479d88e05eb86f453479', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' particularly deadly city in America to drive in, whether rumbling like a freight train', stop_reason=None)], created=1719059633, model='/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=4, total_tokens=20))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8080/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4\",\n",
    "                                      prompt=\"San Francisco is a\")\n",
    "print(\"Completion result:\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec138ef0-9426-481b-923d-11cc7f72ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='cmpl-90e5f537d4604ae488a7088a24d66aaa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure, here's one for you! Why did the tomato turn red?\\n\\nBecause it saw the salad dressing!\", role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1719059734, model='/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=24, total_tokens=47))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8080/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"/home/ubuntu/.cache/modelscope/hub/qwen/Qwen2-0___5B-Instruct-GPTQ-Int4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58a925-76d1-4106-96c6-81104bbd6887",
   "metadata": {},
   "source": [
    "# vLLM Deploying with Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47638f78-5a61-47df-b5e5-d6ebeaba539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 见README"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2",
   "language": "python",
   "name": "qwen2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
